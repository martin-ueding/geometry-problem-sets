% Copyright © 2015 Martin Ueding <dev@martin-ueding.de>

\documentclass[11pt, english, fleqn, DIV=15, headinclude, BCOR=1cm]{scrartcl}

\usepackage[bibatend, color]{../header}

\usepackage{tikz}

\usepackage[tikz]{mdframed}
\newmdtheoremenv[%
    backgroundcolor=black!5,
    innertopmargin=\topskip,
    splittopskip=\topskip,
]{theorem}{Theorem}[section]

\hypersetup{
    pdftitle=
}

\newcounter{totalpoints}
\newcommand\punkte[1]{#1\addtocounter{totalpoints}{#1}}

\newcounter{problemset}
\setcounter{problemset}{2}

\subject{Geometry in Physics}
\ihead{Geometry in Physics -- Problem Set \arabic{problemset}}

\title{Problem Set \arabic{problemset}}

\publishers{Group 1 -- Jens Boos}
\ofoot{Group 1 -- Jens Boos}

\author{
    Martin Ueding \\ \small{\href{mailto:mu@martin-ueding.de}{mu@martin-ueding.de}}
    \and
    Paul Manz \\ \small{\href{mailto:paul.manz@dreiacht.de}{paul.manz@dreiacht.de}}
}
\ifoot{Martin Ueding, Paul Manz}

\ohead{\rightmark}

\usepackage{multicol}

\renewcommand\thesubsection{\thesection.\alph{subsection}}

\begin{document}

\maketitle

\vspace{3ex}

\begin{center}
    \begin{tabular}{rrr}
        problem number & achieved points & possible points \\
        \midrule
        1 & & \punkte{20} \\
        2 & & \punkte{10} \\
        2 & & \punkte{20} \\
        \midrule
        Total & & \arabic{totalpoints}
    \end{tabular}
\end{center}

\section{Linear algebra}

\subsection{Dual vector space}

The dual space $V^*$ is defined as the space of all mappings from the vector
space $V$ to the underlying field, here the reals $\R$:
\[
    V^* = \operatorname{hom}(V, \R).
\]

For $V^*$ to be a vector space, it has to be a group under “$+$” and a group
under “$\cdot$” (except for zero) as well has have two laws of distributivity.
The space of linear maps is a group under “$+$” since it has a neutral element
that maps everything to zero, there is an additive inverse and the addition is
associative. The scalar multiplication is a group on the set except for the
zero vector. Since all the maps are linear, they fulfill linearity and
therefore distributivity.

\subsection{Dual basis}

A basis is defined as a set of linear independent vectors that span the
complete space. The basis $\{\ev_i\}$ of $V$ is already given, so we know that
they are linearly independent and that they generate all of $V$. The dual space
$V^*$ has the same dimension as $V$ since every dimension in $V$ means another
degree of freedom in the mappings $V\to\R$. The basis of $V^*$ is given with
the $\{\ev^i\}$ which fulfill $\ev^i(\ev_j) = \deltaup^i_j$.

We now have to show that this dual basis consists of linear independent
elements and that it spans the whole space. Since we only have $n$ of them,
they are linearly independent if they span the whole $n$-dimensional space. If
they are linearly independent and we have $n$ of them, they must span an
$n$-dimensional space. Showing either will imply the other and therefore imply
that they are the basis of the dual space.

Linear independence of vectors $\ev_i$ means:
\[
    \forall \vec v \in V \colon 
    \sbr{
        v^i \ev_i = \vec 0 \iff \vec v = \vec 0
    }
\]
In the dual space, the vectors are the linear mappings. We take $\vec v \in V$
and $\vec w \in V^*$. Then
\[
    \vec w(\vec v) = w_i v^j \ev^i(\ev_j) = w_i v^j.
\]
Since those act on $V$, this has to be work out for any $\vec v \in V$. For the
$\ev^i$ to be linearly independent, we have to have
\[
    \forall \vec v, \vec w \colon
    \sbr{
        w_i v^j = 0 \iff \vec w = 0
    }.
\]
Since this has to work out for any $\vec v$, this statement is true.
Therefore, the given dual vectors are a basis.

\subsection{Change of basis}

The vectors must not change when the basis changes. The components of the
vectors must change accordingly. If $e^i(e_j) = \deltaup^i_j$ in one basis, it
has to hold that $f^i(f_j) = \deltaup^i_j$ as well. This is sort of a scalar
product, and those scalars must be basis independent.
\begin{align*}
    f^j(f_i)
    &= A^j{}_k e^k\del{\sbr{\sbr{A^{-1}}^\text T}_i{}^m e_m}
    \intertext{%
        That matrix element is a scalar and can be pulled out by the linearity
        of the map.
    }
    &= A^j{}_k \sbr{\sbr{A^{-1}}^\text T}_i{}^m e^k(e_m)
    \intertext{%
        The last part is the Kronecker symbol again, which then sets $m := k$
        in the sum.
    }
    &= A^j{}_k \sbr{\sbr{A^{-1}}^\text T}_i{}^k
    \intertext{%
        This is a matrix times its inverse, which by definition is the unit
        matrix. In components this is
    }
    \deltaup^j_i
\end{align*}
So the scalar product is conserved in this particular choice of transformation
for the dual basis. Any other linear transformation would not conserve the
scalar product for all vectors and dual vectors.

\subsection{Isomorphism}

This isomorphism is probably given by the metric tensor:
\[
    x^i \mapsto x_i = x^j g_{ij}
    \eqnsep
    x_i \mapsto x^i = x_j g^{ij}
\]

The metric tensor depends on the basis and therefore this while expression
depends on the basis.

\subsection{Dual dual space}

A vector from $V$ can be transformed into a vector in $V^*$:
\[
    y_i = x^j g_{ij}.
\]
This can be transformed into a vector into the dual of the dual space:
\[
    z^i = y_k g^{ik} = x^j g_{jk} g^{ik}
\]
The metric tensor contraction $g^{ik} g_{kj}$ gives a $\deltaup^i_j$ in any
basis since all the transforming $\tens A$ will cancel out. Therefore, the dual
dual space is isomorphic to the space itself in any choice of basis.

\subsection{Space of bilinear functionals}

All the functions in $\{f \colon V^* \otimes V \to \R\}$ take on covariant and
one contravariant vector as arguments and produce a scalar. The linear
functionals in $\{h \colon V \to V\}$ each take one contravariant vector and
produce another one. We can explicitly write the things down. Let $f$ and $h$
be from the respective function spaces, $u$ and $v$ from $V$ and $w$ from
$V^*$ as well as $\lambda \in \R$. Then
\[
    f(w, v) = \lambda
    \eqnsep
    h(v) = u.
\]
This is the action of a matrix. One can either let it act on a vector and
obtain another vector or let it act on a covector and a vector and get a
scalar. This matrix is implicitly defined by
\[
    f(w, v) = w A v
    \eqnsep
    h(v) = A v.
\]
Now one can connect the two:
\[
    f(w, v) = w \cdot h(v)
\]

\subsection{Tensor of type (1, 1)}

The dimension of $V$ and $W$ is not given, and they might be different. A A
tensor is built up from a space and its dual, so $V$ and $V^*$. A tensor of
valence $(1, 1)$ is an element from $V^* \otimes V$. So $W$ must be $V$ with
perhaps a different basis. If the dimensions do not match, the linear mapping
cannot be written as a square matrix. So we have to assume that the dimensions
match to proceed. If we take a vector $\ket v$ from $V$ and a covector $u$ from
$V^*$, we can then construct a tensor that provides this mapping as $\ket v
\bra u$. When we apply this to a vector $\ket a$ we get $\ket b = \ket v
\braket{u | a}$ which is a vector in $V \simeq W$.


\subsection{Trace}

The transformation of the matrix $A$ with a transformation $U$
works like so:
\[
    \tilde A = U^{-1} A U
\]
Now to the trace:
\begin{align*}
    \tr(\tilde A)
    &= \tr(U^{-1} A U) \\
    &= [U^{-1}]^i_j A^j_k U^k_i
    \intertext{%
        One can see here that the trace is cyclic.
    }
    &= U^k_i [U^{-1}]^i_j A^j_k \\
    &= \tr(U U^{-1} A) \\
    &= \tr(A)
\end{align*}

\section{Inner product}

\subsection{Definitions}

A non-degenerate tensor provides an isomorphism and can be inverted. It also
means that the provided mapping has a trivial kernel. A symmetric tensor simply
is symmetric in its arguments. And a bilinear tensor is of rank 2.

An example for \emph{all} those classes is any metric tensor, especially the
identity. Non-degenerate tensors are unitary and orthogonal transformations,
symmetric tensors are the Einstein tensor $\tens G$, the energy-stress tensor
$\tens T$. Bilinear tensors are all linear transformations.

\subsection{Canonical isomorphism}

We have used/shown this in 1.d.\ already. The mapping is canonical since it
does not explicitly depend on the basis.

\section{Isomorphy of $S^2$}

\subsection{Parameterization}

To begin with, we do not know anything about the matrix and write
\[
    \tens A = \begin{pmatrix}
        a & c \\ b & d
    \end{pmatrix}
    \eqnsep
    a, b, c, d \in \C.
\]
The non-standard order of the letters is chosen such that it will work out
nicely in the end.

Now a unitary matrix has to fulfill $\tens A^\dagger \tens A = \tens 1$. This
gives us the following condition:
\[
    \tens A^\dagger \tens A
    =
    \begin{pmatrix}
        a^* & b^* \\ c^* & d^*
    \end{pmatrix}
    \begin{pmatrix}
        a & c \\ b & d
    \end{pmatrix}
    =
    \begin{pmatrix}
        |a|^2 + |b|^2 & a^*c + b^*d \\
        c ^*a + d^*b & |c|^2 + |d|^2
    \end{pmatrix}
    \overset !=
    \begin{pmatrix}
        1 & 0 \\ 0 & 1
    \end{pmatrix}.
\]
The unit determinant gives us $ad-bc=1$ as an additional constraint. The two
off-diagonal equations are the complex conjugates of each other and therefore
do not give any new information. Combining one of them with the determinant
equation gives us:
\begin{align*}
    - aa^*c - bb^*c &= b^* \\
    \iff -c \sbr{|a|^2 + |b|^2} &= b^* \\
    \iff c = - b^*.
\end{align*}
We put that back into the determinant equation and see that $d = a^*$. The end
resulting matrix then is
\[
    \tens A = \begin{pmatrix}
        a & -b^* \\ b & a^*
    \end{pmatrix}.
\]

This leaves three real degrees of freedom: $|a|$, $\arg(a)$ and $\arg(b)$.
Since the algebra $\mathfrak{su}(2)$ has three generators, this seems correct.

\subsection{Generator expression}

The three $S_i(\alpha)$ look like exponentiated generator expressions. The
generators used here are:
\[
    T_i = - \iup \eval{\od{S_i}{\alpha}}_{\alpha = 0}.
\]
We use the physicist's convention of hermitian generators. The mathematician's
convention does not have the $- \iup$ and no $\iup$ later on in the
exponential. The generators are then:
\[
    T_1 = \frac12
    \begin{pmatrix}
        0 & 1 \\ 1 & 0
    \end{pmatrix}
    \eqnsep
    T_2 = \frac12
    \begin{pmatrix}
        0 & \iup \\ - \iup & 0
    \end{pmatrix}
    \eqnsep
    T_3 = \frac12
    \begin{pmatrix}
        1 & 0 \\ 0 & 1
    \end{pmatrix}.
\]
They are closely related to the Pauli matrices $\sigma_i$ and the spin
operators $\hat s_i$. Now the $S_i(\alpha)$ can be written as
\[
    S_i(\alpha) = \exp(\iup \alpha T_i)
\]
in the physicist's convention. Apart from the factor of 2, these coincide with
the generators of the algebra $\mathfrak{su}(2)$.

The $\alpha$, $\beta$ and $\gamma$ play a role like the Euler angles in
$\mathrm{SO(3)}$. This has to do with the double cover of $\mathrm{SO(3)}$
which is $\mathrm{SU(2)}$, but that will be shown later on.

Now the actual task we ought to do:
\begin{align*}
    S_3(\alpha) S_2(\beta) S_3(\gamma)
    &=
    \begin{pmatrix}
        \exp\del{\frac\iup2 \alpha} & 0 \\
        0 & \exp\del{-\frac\iup2 \alpha}
    \end{pmatrix}
    \begin{pmatrix}
        \cos\del{\frac\beta2} & - \sin\del{\frac\beta2} \\
        \sin\del{\frac\beta2} & \cos\del{\frac\beta2}
    \end{pmatrix}
    \begin{pmatrix}
        \exp\del{\frac\iup2 \gamma} & 0 \\
        0 & \exp\del{-\frac\iup2 \gamma}
    \end{pmatrix} \\
    \intertext{%
        We will not typeset the matrix calculations, but we did them by hand.
    }
    &=
    \begin{pmatrix}
        \cos\del{\frac\beta2} \exp\del{\frac\iup2 [\alpha + \gamma]} &
        -\sin\del{\frac\beta2} \exp\del{\frac\iup2 [\alpha - \gamma]} \\
        \sin\del{\frac\beta2} \exp\del{\frac\iup2 [- \alpha + \gamma]} &
        \cos\del{\frac\beta2} \exp\del{- \frac\iup2 [\alpha + \gamma]}
    \end{pmatrix}
    \intertext{%
        Now identify
        \[
            a := \cos\del{\frac\beta2} \exp\del{\frac\iup2 [\alpha + \gamma]}
            \eqnsep
            b := \sin\del{\frac\beta2} \exp\del{\frac\iup2 [- \alpha + \gamma]}
        \]
        and it can be seen that this indeed is
    }
    &=
    \begin{pmatrix}
        a & - b^* \\
        b & a^*
    \end{pmatrix}.
\end{align*}
This also fulfills $|a|^2 + |b|^2 = 1$. The angle $\beta$ is needed to get the
modulus of $a$ and $b$ set up, the angles $\alpha$ and $\gamma$ tune the
argument of $a$ and $b$.

There is some redundancy, though. If we add $2\piup$ to $\beta$ and $\gamma$
(or $\alpha$) at the same time, nothing is changed. So although it is true that
every element in $\mathrm{SU(2)}$ can be written with some $\alpha, \beta,
\gamma \in [0, 4\pi)$, it is not true that every element in the group can be
written \emph{uniquely} with some $\alpha, \beta, \gamma$ combination. We think
that this another hint at the double cover.

\subsection{Similarity transformed elements}

\begin{align*}
    U^\dagger \sigma_3 U
    &=
    \begin{pmatrix}
        a^* & b^* \\ -b & a
    \end{pmatrix}
    \sigma_3
    \begin{pmatrix}
        a & -b^* \\ b & a^*
    \end{pmatrix} \\
    &=
    \begin{pmatrix}
        a^* & b^* \\ -b & a
    \end{pmatrix}
    \begin{pmatrix}
        a & -b^* \\ -b & -a^*
    \end{pmatrix} \\
    &=
    \begin{pmatrix}
        a^*a - b^*b & -2 a^* b^* \\
        -2 a b & - a^*a + b^*b
    \end{pmatrix}
    \intertext{%
        This matrix is hermitian as expected. Now we can insert $a$ and $b$
        again.
    }
    &=
    \begin{pmatrix}
        \cos\del{\frac\beta2}^2 - \sin\del{\frac\beta2}^2 &
        - 2 \cos\del{\frac\beta2}\sin\del{\frac\beta2}\exp(-\iup \gamma) \\
        -2 \cos\del{\frac\beta2}\sin\del{\frac\beta2}\exp(\iup\gamma) &
        - \cos\del{\frac\beta2}^2 + \sin\del{\frac\beta2}^2
    \end{pmatrix}
    \intertext{%
        We introduce
        \[
            e := \cos\del{\frac\beta2}^2 - \sin\del{\frac\beta2}^2
            \eqnsep
            f := 2 \cos\del{\frac\beta2}\sin\del{\frac\beta2}\exp(\iup\gamma)
        \]
        and write the matrix more compact as
    }
    &=
    \begin{pmatrix}
        e(\beta) & -f(\beta, \gamma)^* \\ -f(\beta, \gamma) & -e(\beta)
    \end{pmatrix}
\end{align*}

\subsection{Isomorphism to $S^2$}

The set $\mathscr A$ only contains unique matrices, since it is a set. That
way, the parameter ranges of the three angles do not matter since only unique
matrices are kept. The two-sphere is two-dimensional, and therefore two
parameters are sufficient to describe a point on the $S^2$. Sensible parameter
ranges are $\beta \in [0, 2\piup)$ and $\gamma \in [0, \piup)$ to get all
possible values for $e$ and $f$. A point on $S^2$ can be described but the two
angles $\theta$ and $\phi$.

So we could give the following mapping:
\begin{gather*}
    S^2 \subset \R^3 \to \mathscr A, \\
    \begin{pmatrix}
        \sin(\theta) \cos(\phi) \\
        \sin(\theta) \sin(\phi) \\
        \cos(\theta)
    \end{pmatrix}
    \mapsto
    \begin{pmatrix}
        \cos\del{\frac\phi2}^2 - \sin\del{\frac\phi2}^2 &
        - 2 \cos\del{\frac\phi2}\sin\del{\frac\phi2}\exp(-\iup \theta) \\
        -2 \cos\del{\frac\phi2}\sin\del{\frac\phi2}\exp(\iup\theta) &
        - \cos\del{\frac\phi2}^2 + \sin\del{\frac\phi2}^2
    \end{pmatrix}
\end{gather*}

This is a $1:1$ mapping, but does that suffice to show that $\mathscr A \simeq
S^2$?

\end{document}

% vim: spell spelllang=en tw=79
