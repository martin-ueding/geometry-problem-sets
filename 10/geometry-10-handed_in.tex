% Copyright Â© 2015 Martin Ueding <dev@martin-ueding.de>

\documentclass[11pt, english, fleqn, DIV=15, headinclude, BCOR=1cm]{scrartcl}

\usepackage[bibatend]{../header}

\usepackage{tikz}

\usepackage[tikz]{mdframed}
\newmdtheoremenv[%
    backgroundcolor=black!5,
    innertopmargin=\topskip,
    splittopskip=\topskip,
]{theorem}{Theorem}[section]

\usepackage{../my-boxes}

\usepackage{enumerate}

\hypersetup{
    pdftitle=
}

\newcounter{totalpoints}
\newcommand\punkte[1]{#1\addtocounter{totalpoints}{#1}}

\newcounter{problemset}
\setcounter{problemset}{10}

\subject{Geometry in Physics}
\ihead{Geometry in Physics -- Problem Set \arabic{problemset}}

\title{Problem Set \arabic{problemset}}

\publishers{Group 1 -- Jens Boos}
\ofoot{Group 1 -- Jens Boos}

\author{
    Martin Ueding \\ \small{\href{mailto:mu@martin-ueding.de}{mu@martin-ueding.de}}
    \and
    Paul Manz \\ \small{\href{mailto:paul.manz@dreiacht.de}{paul.manz@dreiacht.de}}
}
\ifoot{Martin Ueding, Paul Manz}

\ohead{\rightmark}

\usepackage{multicol}

\renewcommand\thesubsection{\thesection.\alph{subsection}}

\begin{document}

\maketitle

\vspace{3ex}

\begin{center}
    \begin{tabular}{rrr}
        problem & achieved points & possible points \\
        \midrule
        \nameref{homework:1} & & \punkte{10} \\
        \nameref{homework:2} & & \punkte{20} \\
        \nameref{homework:3} & & \punkte{20} \\
        \midrule
        Total & & \arabic{totalpoints}
    \end{tabular}
\end{center}

\section{Lie algebra}
\label{homework:1}

\subsection{Matrix commutator}

\paragraph{Linearity}

The matrix group $\GL(n, \R)$ where we get our matrices from is a vector space.
The commutator of matrices therefore is linear in the arguments:
\begin{align*}
    [\lambda \mat A + \mu \mat B, \mat C]
    &= [\lambda \mat A + \mu \mat B] \mat C - \mat C [\lambda \mat A + \mu \mat B]
    \intertext{%
        The matrix multiplication is associative and also has the distributive
        laws.
    }
    &= \lambda \mat A \mat C + \mu \mat B \mat C - \mat C \mu \mat B
    - \mat C \lambda \mat A
    \intertext{%
        Then we move the factors up front and reorder the summands.
    }
    &= \lambda \mat A \mat C - \lambda \mat C \mat A
    + \mu \mat B \mat C - \mu \mat C \mat B
    \intertext{%
        We can factor out the factors and get recognize the matrix commutators
        again.
    }
    &= \lambda [\mat A, \mat C] + \mu [\mat B, \mat C]
\end{align*}
The linearity in the second argument follows from the anti-commutativity of the
matrix commutator.

\paragraph{Anti-commutativity}

The way the commutator is defined it is easy to see that it is anti-symmetric
in its arguments.

\paragraph{Jacobi identity}

We expand the inner commutator.
\begin{gather*}
    [\mat A, [\mat B, \mat C]]
    + [\mat C, [\mat A, \mat B]]
    + [\mat B, [\mat C, \mat A]]
    = [\mat A , \mat B \mat C -\mat C \mat B ]
    + [\mat C , \mat A \mat B -\mat B \mat A ]
    + [\mat B , \mat C \mat A -\mat A \mat C ]
    \intertext{%
        We expand the outer commutator.
    }
    \quad = \mat A \mat B \mat C 
    - \mat A \mat C \mat B 
    - \mat B \mat C \mat A 
    + \mat C \mat B \mat A 
    + \mat C \mat A \mat B 
    - \mat C \mat B \mat A 
    - \mat A \mat B \mat C 
    + \mat B \mat A \mat C 
    + \mat B \mat C \mat A 
    - \mat B \mat A \mat C 
    - \mat C \mat A \mat B 
    + \mat A \mat C \mat B 
    \intertext{%
        Then we sort the terms.
    }
    \quad =
    \mat A \mat B \mat C 
    - \mat A \mat B \mat C 
    + \mat A \mat C \mat B 
    - \mat A \mat C \mat B 
    + \mat B \mat A \mat C 
    - \mat B \mat A \mat C 
    + \mat B \mat C \mat A 
    - \mat B \mat C \mat A 
    + \mat C \mat A \mat B 
    - \mat C \mat A \mat B 
    + \mat C \mat B \mat A 
    - \mat C \mat B \mat A 
    \intertext{%
        And they all cancel each other.
    }
    \quad = 0
\end{gather*}

\subsection{Cross product}

\paragraph{Linearity}

Using components one can lead this back to the linearity of a bilinear form:
\[
    \vec a \times \vec b = \epsilon_{ijk} a^i b^j \ev^k.
\]
This expression is linear in $a^i$ and $b^j$ each.

\paragraph{Anti-commutativity}

If we exchange $\vec a$ and $\vec b$ in the above expression, we have to
exchange the indices in the Levi-Civita symbol and therefore get a minus sign.
\begin{align*}
    \vec a \times \vec b
    &= \epsilon_{ijk} a^i b^j \ev^k
    \intertext{%
        We exchange the indices of the Levi-Civita symbol.
    }
    &= - \epsilon_{jik} a^i b^j \ev^k
    \intertext{%
        For consistency, we can also exchange the components of $\vec a$ and
        $\vec b$ here. Those are just regular numbers and therefore do not
        cause a change in sign.
    }
    &= - \epsilon_{jik} b^j a^i \ev^k
    \intertext{%
        By the definition, this is a cross product in the other order.
    }
    &= - \vec b \times \vec a
\end{align*}

\paragraph{Jacobi identity}

We will need to write it out again here. This time we get to do some gymnastics
with Levi-Civita symbols. We first expand everything in terms of components.
\begin{gather*}
    \vec a \times [\vec b \times \vec c]
    + \vec c \times [\vec a \times \vec b]
    + \vec b \times [\vec c \times \vec a] \\
    \quad = \epsilon_{ijk} a^i \epsilon^j{}_{lm} b^l c^m \ev^k
    + \epsilon_{ijk} c^i \epsilon^j{}_{lm} a^l b^m \ev^k
    + \epsilon_{ijk} b^i \epsilon^j{}_{lm} c^l a^m \ev^k \\
    \intertext{%
        We can factor out the Levi-Civita symbols and the basis vector.
    }
    \quad = \epsilon_{ijk} \epsilon^j{}_{lm} \sbr{
        a^i b^l c^m
        + c^i a^l b^m
        + b^i c^l a^m
    } \ev^k \\
    \intertext{%
        Then we can contract both symbols and obtain some metric tensors.
    }
    \quad = \sbr{g_{kl} g_{im} - g_{km} g_{li}} \sbr{
        a^i b^l c^m
        + c^i a^l b^m
        + b^i c^l a^m
    } \ev^k \\
    \intertext{%
        We apply those metric tensors and lower some indices with them.
    }
    \quad = \sbr{
        a^i b^k c_i
        + c^i a^k b_i
        + b^i c^k a_i
        - a^i b_i c^k
        - c^i a_i b^k
        - b^i c_i a^k
    } \ev_k \\
    \intertext{%
        The terms need to be reordered.
    }
    \quad = \sbr{
        a^i b^k c_i
        - a_i b^k c^i
        + a^k b_i c^i
        - a^k b^i c_i
        + a_i b^i c^k
        - a^i b_i c^k
    } \ev_k \\
    \intertext{%
        And now the terms cancel and we get zero.
    }
    \quad = 0
\end{gather*}

\section{Exponential map}
\label{homework:2}

\begin{enumerate}[(a)]
    \item
        Using the chain rule and using that a matrix will commute with a smooth
        function of itself, the desired property
        \[
            \od{}t \exp(t \mat A) = \mat A \exp(t \mat A)
        \]
        directly follows. But one can use the definition to show this as well,
        of course:
        \begin{align*}
            \od{}t \exp(t \mat A)
            &= \od{}t \sum_{k = 0}^\infty \frac{[t \mat A]^k}{k!}
            \intertext{%
                The differentiation with respect to $t$ is a limit, the sum is
                also a limit. Since the exponential function is one of the
                smoothest and most differentiable functions known to
                Physicists, it should be possible to interchange those two
                limits.
            }
            &= \sum_{k = 0}^\infty \od{}t \frac{t^k \mat A^k}{k!}
            \intertext{%
                We apply the derivative. The case $k = 0$ will give zero, so we
                exclude that from the summation.
            }
            &= \sum_{k = 1}^\infty \frac{t^{k-1} \mat A^k}{[k-1]!}
            \intertext{%
                We can move one $\mat A$ to the front and relabel the summation
                index $n = k-1$.
            }
            &= \mat A \sum_{n = 0}^\infty \frac{t^n \mat A^n}{n!}
            \intertext{%
                Now we can use the definition of the exponential function again
                and wrap this up.
            }
            &= \mat A \exp(t \mat A)
        \end{align*}

    \item
        We have $\mat A \mat B = \mat B \mat A$, they commute.
        This is our first approach, but we got stuck a bit later.
        To show all the
        steps, we again start by expanding the exponential functions.
        \begin{align*}
            \exp(\mat A + \mat B)
            &= \sum_{k = 0}^\infty \frac{[\mat A + \mat B]^k}{k!}
            \intertext{%
                Now we need to use the binomial theorem to split this sum.
                Since $\mat A$ and $\mat B$ commute, we can actually do this
                and do not have to carry $2^k$ individual terms.
            }
            &= \sum_{k = 0}^\infty \sum_{p = 0}^k \binom kp \frac{\mat A^p \mat B^{k-p}}{k!}
            \intertext{%
                We expand the binomial coefficient.
            }
            &= \sum_{k = 0}^\infty \sum_{p = 0}^k \frac{k!}{[k-p]! p!}
            \frac{\mat A^p \mat B^{k-p}}{k!}
            \intertext{%
                Now we can cancel the factorial from the original exponential
                series. Then we can write this in a suggestive way.
            }
            &= \sum_{k = 0}^\infty \sum_{p = 0}^k \frac{\mat A^p}{p!}
            \frac{\mat B^{k-p}}{[k-p]!}
            \intertext{%
                This is our weakest part of our derivation. We would argue that
                by this double sum all powers of $\mat A$ and $\mat B$ occur
                exactly once with their corresponding factorial. Therefore we
                can reorder the summations completely and just write this with
                new indices as
            }
            &= \sum_{k = 0}^\infty \sum_{l = 0}^\infty
            \frac{\mat A^k}{k!} \frac{\mat B^l}{l!}.
            \intertext{%
                Then we can factor by moving the summation sign.
            }
            &= \sum_{k = 0}^\infty \frac{\mat A^k}{k!}
            \sum_{l = 0}^\infty \frac{\mat B^l}{l!}
            \intertext{%
                And this is the definition again and we get our final result.
            }
            &= \exp(\mat A) \exp(\mat B)
        \end{align*}

        There is another neat derivation which is based on a trick we had to
        find first. Using the result of (a) we can start with an auxiliary
        function $F$.
        \[
            F(t)
            := \exp([\mat A + \mat B] t) - \exp(\mat At) \exp(\mat B t)
        \]
        We differentiate with respect to $t$ which is usable as of part~(a).
        Then we obtain:
        \begin{align*}
            \dot F(t)
            &= [\mat A + \mat B] \exp([\mat A + \mat B] t)
            - \mat A \exp(\mat At) \exp(\mat B t)
            - \exp(\mat At) \mat B \exp(\mat B t)
            \intertext{%
                We can factor the exponentials in the last two terms and
                combine that with the very first term.
            }
            &= [\mat A + \mat B] \sbr{\exp([\mat A + \mat B] t)
            - \exp(\mat At) \exp(\mat B t)}
            \intertext{%
                And then we can recognize the function $F$ again.
            }
            &= [\mat A + \mat B] F(t)
        \end{align*}
        The only sensible solution to this differential equation is $F(t) = 0$.
        This then implies that
        \[
            \exp([\mat A + \mat B] t) = \exp(\mat At) \exp(\mat B t)
        \]

        If $\mat A$ and $\mat B$ do not commute, this relation does not hold
        since one could not apply the binomial theorem. There will be a third
        exponential with the commutators and higher order commutators. That is
        the Baker-Campbell-Hausdorff formula.

    \item
        Use the previous result with $\mat A = s \mat A$ and $\mat B = t \mat
        A$.

    \item
        We have a matrix from $\GL(n, \R)$, so we cannot say that it can be
        diagonalized. There always exists the Jordan form which has the
        eigenvalues on the diagonal. Since the result will not differ, we will
        assume that $\mat A$ is diagonalizable. Then we can find an eigenvalue
        matrix $\mat \Lambda$ and a transform $\mat S$ such that $\mat \Lambda
        = \mat S \mat A \mat S\inv$.
        \begin{align*}
            \det\del{\exp(\mat A)}
            &= \det\del{\exp(\mat S\inv \mat \Lambda \mat S)}
            \intertext{%
                Using part (g) of this problem, we can pull out $\mat S$.
            }
            &= \det\del{\mat S\inv \exp(\mat \Lambda) \mat S}
            \intertext{%
                The determinant of a product is the product of the
                determinants.
            }
            &= \det \mat S\inv \det\del{\exp(\mat \Lambda)} \det \mat S
            \intertext{%
                The determinants of the transform will just cancel.
            }
            &= \det\del{\exp(\mat \Lambda)}
            \intertext{%
                The eigenvalue matrix diagonal, so we just have a matrix with
                the exponentiated eigenvalues on the diagonal. The determinant
                of such a diagonal matrix is just the product of all the
                elements on the diagonal.
            }
            &= \prod_i \exp(\lambda_i)
            \intertext{%
                We can combine those exponentials.
            }
            &= \exp(\sum_i \lambda_i)
            \intertext{%
                And that is the trace of the matrix $\mat \Lambda$.
            }
            &= \exp(\tr \mat \Lambda)
            \intertext{%
                Now we add the $\mat S$ again.
            }
            &= \exp\del{\tr(\mat S \mat S\inv \mat \Lambda)}
            \intertext{%
                The trace is cyclic, so we can move the $\mat S$ to the other
                end.
            }
            &= \exp\del{\tr(\mat S\inv \mat \Lambda \mat S)}
            \intertext{%
                This expression is $\mat A$.
            }
            &= \exp(\tr \mat A)
        \end{align*}
        For the general case one would have to use the Jordan form and its
        powers, but the idea is the same.

    \item
        Using the previous result, having $\det\del{\exp(\mat A)} = 0$ would
        mean that $\exp(\tr \mat A) = 0$. However, the real matrix cannot have
        a trace such that the exponential of it would be zero. This would not
        even work with a complex matrix.

    \item
        We start with the definition of the inverse in general.
        \begin{align*}
            \exp(\mat A) \exp(\mat A)\inv &= \mat 1
            \intertext{%
                Then we write the unit matrix as an exponentiated zero matrix.
                With the exponential map of the Lie algebra in mind this just
                means that we take an arbitrary generator and set the parameter
                vector to zero.
            }
            \exp(\mat A) \exp(\mat A)\inv &= \exp(\mat 0)
            \intertext{%
                Now we just use the definition of the additive inverse.
            }
            \exp(\mat A) \exp(\mat A)\inv &= \exp(\mat A - \mat A)
            \intertext{%
                As we have shown in (a), we can split the sum inside the
                exponential into the product of two exponentials.
            }
            \exp(\mat A) \exp(\mat A)\inv &= \exp(\mat A) \exp(- \mat A)
            \intertext{%
                And then we cancel the first exponential and get the desired
                expression.
            }
            \exp(\mat A)\inv &= \exp(- \mat A)
        \end{align*}

    \item
        We can move the $\mat S$ and $\mat S\inv$ into the power since the
        $\mat S\inv S$ pairs in between the $\mat A$ will just cancel. See
        $[\mat S \mat A \mat S\inv]^2 = \mat S \mat A \mat S\inv \mat S \mat A
        \mat S\inv = \mat S \mat A^2 \mat S\inv$.
        \[
            \mat S \exp(\mat A) \mat S\inv
            = \sum_{k = 0}^\infty \mat S \frac{\mat A^k}{k!} \mat S\inv
            = \sum_{k = 0}^\infty \frac{[\mat S \mat A \mat S\inv]^k}{k!}
            = \exp(\mat S \mat A \mat S\inv)
        \]
        This would break if $\mat S$ would be singular, so we needed to have
        $\det(\mat S) \neq 0$.

    \item
        The transpose changes the order in a product, but that does not hurt
        since we only have powers of $\mat A$ here:
        \begin{align*}
            \exp(\mat A^\T)
            &= \sbr{\sum_{k = 0}^\infty \frac{[\mat A^\T]^k}{k!}}
            \intertext{%
                We can move the transpose outside of the bracket since the
                reversal in the order of the $k$ factors of $\mat A$ does not
                change anything.
            }
            &= \sbr{\sum_{k = 0}^\infty \frac{[\mat A^k]^\T}{k!}}
            \intertext{%
                We can then use the linearity of the transpose and move it out
                of the sum.
            }
            &= \sbr{\sum_{k = 0}^\infty \frac{\mat A^k}{k!}}^\T
            \intertext{%
                And then we recover the other side of the desired equation.
            }
            &= \sbr{\exp(\mat A)}^\T
        \end{align*}

    \item
        This is very similar to (d). We can just diagonalize the matrix $\mat
        A$ in the exponential and have a diagonal eigenvalue matrix $\mat
        \Lambda$, where $\exp(\mat \Lambda)$ then contains the exponentiated
        eigenvalues of $\mat A$. And any matrix can be diagonalized over the
        complex numbers.
\end{enumerate}

\section{Lie algebra 2}
\label{homework:3}

\subsection{Basis}

The matrices $\mat S_i$ are the generators, but in the mathematician's
convention of anti-hermitian generators. This means that there are no
additional imaginary units are build in everywhere.

There are three parameters here: $a$, $\Re z$ and $\Im z$ or equivalently $a$,
$z$ and $\overline z$. To get the generators we want we choose the first
parametrization of the Lie algebra. Then we can compute the generators for the
general $\su(2)$ algebra element $\mat s(a, \Re z, \Im z)$:
\[
    \mat S_i = \od{\mat s}{\alpha^i}(0)
\]
where $\vec \alpha$ is the parameter vector. The physicist's convention would
add $-\iup$ to this equation. The general element is written
\[
    \mat s(a, \Re z, \Im z) =
    \begin{pmatrix}
        \iup a & - \Re z + \iup \Im z \\
        \Re z + \iup \Im z & - \iup a \\
    \end{pmatrix}.
\]
Taking those derivatives is straightforward. The additional factor of one half
is just a convention that will make the structure constants in (b) nicer and
will set the scaling factor in the scalar product for generators. We are not
going to show the three matrix derivatives here explicitly since they now
should be straightforward to read off.

\subsection{Lie product}

We have to derive the structure constants here. As physicists we see their
relation to the Pauli matrices as $\mat S_i = - \half \iup \mat \sigma_i$. We
know the structure constants for the Pauli matrix algebra, which are $2 \iup
\epsilon_{ij}{}^k$. Inserting all this gives Equation~(11) from the problem
set.

\subsection{Universal covering group}

The exponentiation of $\su(2)$ should yield the universal covering group, which
is isomorphic to $\SU(2)$ and $\SO(3) \times \Z_2$. The general element from
the universal covering group looks like this in the mathematician's convention:
\[
    \mat B(\vec \alpha) = \exp(\alpha^i \mat S_i).
\]
The generators $\mat S_i$ have no trace, which means that the matrices $\mat B$
we get from the exponentiation will have unit determinant. We therefore will
have some special matrix group in the end.

So let us do this calculation. One can approach this with an Euler angle
prescription where we do not exponentiate the full expression $\alpha^i \mat
S_i$ at once, but exponentiate each generate on its own and matrix multiply the
results.
\begin{align*}
    \mat B(\vec \alpha)
    &= \prod_{i = 1}^3 \exp(\alpha^i \mat S_i) \qquad \text{no summation convention} \\
    &= \exp(\alpha^1 \mat S_1) \exp(\alpha^2 \mat S_2) \exp(\alpha^3 \mat S_3)
    \intertext{%
        All those matrix exponentials work in the same way. The generator
        squared gives something proportional to the negative unit matrix.
        Therefore the terms in the power series of the exponential fall into
        even and odd terms and can then be grouped using sine and cosine
        functions. The last one is easy since it is a diagonal matrix. We
        obtain with a help from Mathematica
    }
    &= \begin{pmatrix}
    \cos\del{\frac{\alpha_1}2} & \iup \sin\del{\frac{\alpha_1}2} \\
    \iup \sin\del{\frac{\alpha_1}2} & \cos\del{\frac{\alpha_1}2} \\
    \end{pmatrix}
    \begin{pmatrix}
    \cos\del{\frac{\alpha_2}2} & - \sin\del{\frac{\alpha_2}2} \\
    \sin\del{\frac{\alpha_2}2} & \cos\del{\frac{\alpha_2}2} \\
    \end{pmatrix}
    \begin{pmatrix}
        \exp\del{\iup \frac{\alpha_3}2} & 0 \\
        0 & \exp\del{\iup \frac{\alpha_3}2}
    \end{pmatrix}.
\end{align*}
Then we build up the product of those three matrices. The matrix is rather
big, so we list it here in components:
\begin{align*}
    B_{11} &= \exp\del{\iup \frac{\alpha_3}2} \sbr{\cos\del{\frac{\alpha_1}2} \cos\del{\frac{\alpha_2}2} + \iup \sin\del{\frac{\alpha_1}2} \sin\del{\frac{\alpha_2}2}} \\
    B_{21} &= \sbr{\half + \frac\iup2} \exp\del{- \iup \frac{\alpha_3}2}
    \sbr{\sin\del{\frac{\alpha_1-\alpha_2}2} + \iup \sin\del{\frac{\alpha_1 +
    \alpha_2}2} } \\
    B_{12} &= \exp\del{\iup \frac{\alpha_3}2} \sbr{\iup
\sin\del{\frac{\alpha_1}2} \cos\del{\frac{\alpha_2}2} +
\cos\del{\frac{\alpha_1}2} \sin\del{\frac{\alpha_2}2}} \\
    B_{22} &= \exp\del{- \iup \frac{\alpha_3}2} \sbr{\cos\del{\frac{\alpha_1}2}
\cos\del{\frac{\alpha_2}2} - \iup \sin\del{\frac{\alpha_1}2}
\sin\del{\frac{\alpha_2}2}}
\end{align*}

One can check that this matrix is unitary, so we can conclude that this matrix
comes from $\SU(2)$.

\end{document}

% vim: spell spelllang=en tw=79
